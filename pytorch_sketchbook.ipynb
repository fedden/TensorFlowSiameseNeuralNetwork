{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.utils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from dataset import next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "  \n",
    "  def __init__(self, \n",
    "               dropout=0.2, \n",
    "               output_size=5):\n",
    "    \n",
    "    super(SiameseNetwork, self).__init__()\n",
    "    \n",
    "    self.dropout = dropout\n",
    "    self.output_size = output_size\n",
    "    \n",
    "    self.cnn = nn.Sequential(\n",
    "        nn.ReflectionPad2d(1),\n",
    "        nn.Conv2d(1, 4, kernel_size=3),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(4),\n",
    "        nn.Dropout2d(p=dropout),\n",
    "        \n",
    "        nn.ReflectionPad2d(1),\n",
    "        nn.Conv2d(4, 8, kernel_size=3),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(8),\n",
    "        nn.Dropout2d(p=dropout),\n",
    "        \n",
    "        nn.ReflectionPad2d(1),\n",
    "        nn.Conv2d(8, 8, kernel_size=3),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(8),\n",
    "        nn.Dropout2d(p=dropout),\n",
    "    )\n",
    "    \n",
    "    self.dense = nn.Sequential(\n",
    "        nn.Linear(8 * 100 * 100, 500),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(500, 500),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(500, output_size) \n",
    "    )\n",
    "    \n",
    "  def forward_once(self, x):\n",
    "    output = self.cnn(x)\n",
    "    output = output.view(output.size()[0], -1)\n",
    "    return self.dense(output)\n",
    "  \n",
    "  \n",
    "  def forward(self, input_1, input_2):\n",
    "    output_1 = self.forward_once(input_1)\n",
    "    output_2 = self.forward_once(input_2)\n",
    "    return output_1, output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "  \"\"\"\n",
    "  Contrastive loss function.\n",
    "  Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, margin=2.0):\n",
    "    super(ContrastiveLoss, self).__init__()\n",
    "    self.margin = margin\n",
    "\n",
    "  def forward(self, output1, output2, label):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                                  (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "    return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_var(x):\n",
    "  if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "  return Variable(x)\n",
    "\n",
    "\n",
    "def denorm(x):\n",
    "  out = (x + 1) / 2\n",
    "  return out.clamp(0, 1)\n",
    "\n",
    "\n",
    "def imshow(img, text=None, size=8):\n",
    "  \n",
    "  np_img = img.numpy()\n",
    "  width, height = np_img.shape\n",
    "  \n",
    "  fig = plt.figure(figsize=(size * height / width, size))\n",
    "  \n",
    "  plt.axis(\"off\")\n",
    "  if text:\n",
    "    plt.text(75, 8, text, style='italic', fontweight='bold',\n",
    "             bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "  plt.imshow(np_img)\n",
    "  plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py:97: UserWarning: \n",
      "    Found GPU0 Quadro M1000M which is of cuda capability 5.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 296 image(s) found.\n",
      "Output directory set to ./sign_dataset/kfh/output.Initialised with 370 image(s) found.\n",
      "Output directory set to ./sign_dataset/acorn/output.Initialised with 249 image(s) found.\n",
      "Output directory set to ./sign_dataset/kallars/output.Initialised with 846 image(s) found.\n",
      "Output directory set to ./sign_dataset/hamptons/output.Initialised with 435 image(s) found.\n",
      "Output directory set to ./sign_dataset/bryan_keegan/output.Initialised with 1014 image(s) found.\n",
      "Output directory set to ./sign_dataset/ocean/output.Initialised with 342 image(s) found.\n",
      "Output directory set to ./sign_dataset/barnard_marcus/output."
     ]
    }
   ],
   "source": [
    "dataset_folder = './sign_dataset/'\n",
    "\n",
    "embedding_size = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "training_iterations = 5000\n",
    "save_step = 100\n",
    "batch_size = 4\n",
    "\n",
    "image_height = 64\n",
    "image_width = 64\n",
    "image_channels = 3\n",
    "target_shape = (image_height, image_width, image_channels)\n",
    "\n",
    "amount_test_batches = 20\n",
    "amount_test_images = batch_size * amount_test_batches\n",
    "\n",
    "batch_generator = next_batch(batch_size,\n",
    "                             data_directory=dataset_folder,\n",
    "                             target_shape=target_shape,\n",
    "                             probability=0.1)\n",
    "\n",
    "# Load model and loss.\n",
    "if torch.cuda.is_available():\n",
    "    network   = SiameseNetwork(output_size=embedding_size).cuda()\n",
    "    criterion = ContrastiveLoss().cuda()\n",
    "    optimiser = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "    for batch_left, batch_right, batch_similar in batch_generator:\n",
    "\n",
    "        # Initialise the gradients buffers.\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Convert to variables on the GPU.\n",
    "        batch_left, batch_right, batch_similar = \\\n",
    "            torch.from_numpy(batch_left), torch.from_numpy(batch_right), torch.from_numpy(batch_similar)\n",
    "        batch_left, batch_right, batch_similar = \\\n",
    "            to_var(batch_left), to_var(batch_right), to_var(batch_similar)\n",
    "\n",
    "        # Get encoding vectors.\n",
    "        encoding_left, encoding_right = network(batch_left, batch_right)\n",
    "\n",
    "        # Loss calculated and backpropagated.\n",
    "        loss = criterion(encoding_left, encoding_right, is_different)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        print(\"loss: {}\".format(loss.data[0]))\n",
    "\n",
    "else:\n",
    "    print(\"Please run on hardware with GPU support\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
